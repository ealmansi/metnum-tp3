\subsection{Base de datos MNIST}

Para implementar y poner a prueba el sistema se utilizó la base de datos MNIST de dígitos manuscritos\footnote{Se puede obtener desde el sitio http://yann.lecun.com/exdb/mnist/.}. Esta consta de un conjunto de entrenamiento y un conjunto de prueba de 60.000 y 10.000 imágenes, respectivamente. Cada una de ellas viene apareada con una etiquetada indicando el dígito que contiene.

Las imágenes tienen un tamaño estandarizado de $28 \times 28$ píxeles, se encuentran libres de ruido, debidamente centradas, y tienen un formato color en escala de grises de 8 bits. Estas características permiten enfocar el estudio en el reconocimiento de los dígitos, sin tener que realizar preprocesamiento y condicionamiento de las imágenes, lo cual suele ser una etapa relevante en el problema de OCR.

Según el enfoque propuesto en la sección \ref{intro:ocr}, el conjunto de entrenamiento se puede considerar como una matriz de datos \decMat{\X}{\M}{\N} donde $\M = 60000$ y $\N = 28 * 28 = 784$. De esta forma, la matriz de covarianza será \decMat{\frac{1}{\N - 1} \X^t \X}{\N}{\N}.

\subsection{Aplicabilidad del Método de las potencias}

Para utilizar el Método de las potencias combinado con deflación fue necesario verificar primero que se cumplieran las hipótesis sobre las cuales se basan. Por un lado, la matriz de covarianza es simétrica por construcción, por lo cual sus autovectores serán necesariamente ortogonales y por lo tanto el proceso de deflación funciona correctamente. Por otro lado, la condición de que los autovalores fueran distintos en módulo no se pudo demostrar de forma teórica ya que es posible generar un conjunto de datos donde esto no se cumpla. Sin embargo, se corroboró empíricamente que esta condición efectivamente se verifica en el rango de autovalores de mayor valor absoluto, que son los de interés para esta aplicación (en contraposición a las direcciones principales de menor relevancia, las cuales se encuentran asociadas a autovalores muy similares y cercanos a cero).

Las características del método lo hicieron particularmente apto para esta aplicación, ya que solo fue necesario computar un subconjunto de autovectores dominantes de la matriz de covarianza. Por esta razón, fue posible aplicar el método tantas veces como componentes principales se desearan, intercalando el proceso de deflación entre distintas iteraciones. Este esquema es inherentemente más eficiente que otros métodos para calcular todos los autovectores de la matriz de forma simultánea, como el algoritmo QR en su versión ordinaria.

\subsubsection{Criterio de parada}

Si llamamos $v_i$ con $i = 0, 1, 2 ...$ a la aproximación generada por la i-ésima iteración, el criterio se parada consiste en considerar que el algoritmo ha convergido cuando se cumple:
$$ 1\, - \left | <v_{i-1}, v_{i}> \right | \, \leq \delta $$
donde $\delt \in \left ( 0, 1 \right )$ es el parámetro de la implementacion que determina la precisión en el cálculo de los autovectores.

Dado que ambos vectores son unitarios por construcción, el valor absoluto de su producto interno es $\leq 1$ \footnote{Por la desigualdad de Cauchy–Schwarz-Bunyakovsky}, y será más cercano a $1$ cuando la dirección de $v_{i-1}$ y de $v_{i}$ sean más próximas entre sí. De esta forma, cuanto menor sea \delt, más restrictiva será la condición, requiriendo que los vectores sean colineales para $\delt = 0$. Esto significa que se terminará el proceso iterativo cuando sucesivas aproximaciones no perciban grandes variaciones en su dirección.

\subsection{Dimensión del espacio de llegada de la transformación}

Para determinar la cantidad de dimensiones apropiada para la transformación, se buscó un balance entre velocidad de ejecución y rendimiento. Por un lado, el cálculo de autovectores por medio del Método de las potencias se vuelve más lento cada vez que se realiza una iteración, ya que los autovalores difieren menos significativamente para componentes de menor relevancia. Por otro lado, si se descartan demasiadas dimensiones, inevitablemente se perderá cierto grado de información y es esperable que la calidad del reconocimiento decremente.

Para determinar un criterio, se determinó para cuál cantidad de componentes principales se acumula más del $99\%$ de la varianza de los datos. Realizando el cociente entre la varianza acumulada en las primeras 350 componentes y la varianza total, se observa una proporción mayor a 0.99, por lo cual todos los experimentos para determinar la influencia de la dimensión en el rendimiento se realizaron con valores de $k$ entre 1 y 350.

\subsection{Criterio de clasificación y experimentación}

Una vez obtenida la matriz de transformación, el mecanismo de clasificación se realizó de la siguiente manera.

En primer lugar, se transformó todo el conjunto de entrenamiento, y se computó el promedio de las imágenes transformadas de cada dígito. De esta forma, se obtiene un representante de cada clase en el espacio de llegada de la transformación.

Luego, el reconocimiento de una imagen desconocida se realiza simplemente aplicando la transformación sobre la nueva observación, y verificando a cuál representante resulta más cercano el nuevo dato según la norma euclídea. El dígito representado por el promedio más próximo se declara como el más probable para la imagen que se quiere reconocer.

Finalmente, una vez implementado el sistema, se experimentó sobre el parámetro de precisión \delt y la cantidad de componentes principales utilizadas al representar los datos (valor de $k$). Para determinar la influencia de estos parámetros sobre el rendimiento, se ejecutó el proceso de reconocimiento sobre todo el conjunto de imágenes de prueba de la base de datos para varias combinaciones de \delt y $k$, midiendo la cantidad de instancias clasificadas correctamente en cada caso según las etiquetas provistas.