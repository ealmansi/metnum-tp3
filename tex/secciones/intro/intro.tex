\subsection{OCR y reducción de dimensionalidad}
\label{intro:ocr}

El reconocimiento óptico de caracteres (\emph{Optical Character Regonition}) es la interpretación automatizada de texto en formato de imágen, y la conversión mecánica de texto manuscrito o impreso a versión digital. Una variante más restringida del problema se enfoca en el reconocimiento de dígitos manuscritos, siendo ese el foco de este trabajo.

El enfoque utilizado para resolver el problema consiste, en primer lugar, en considerar a cada imagen de un dígito como una instancia de observación sobre \N variables, donde \N es la cantidad de píxeles que la componen. De esta forma, un conjunto de \M imágenes se puede interpretar como un conjunto de datos con \M muestras sobre \N variables.

Por ejemplo, si se tiene un conjunto de imágenes de tamaño $\n * \n = \N$ en escala de grises de 8 bits, se considera a cada píxel como una variable que toma valores entre 0 y 255, y cada una de las imágenes será una muestra con una observación para cada una de las \N variables.

Ahora bien, bajo esta interpretación, es evidente que no todas las variables tienen la misma relevancia a la hora de diferenciar una muestra de la otra; aquellos píxeles que no pertenezcan a la forma habitual de ninguno de los diez dígitos, tendrán valores similares o idénticos en todas las muestras. Por el contrario, algunos píxeles se activarán para ciertos dígitos y no para los demás (o lo que es igual, esas variables tomarán valores distintos en las muestras de unos u otros  dígitos), permitiendo caracterizar y distinguir distintas clases dentro de las observaciones.

El análisis de componentes principales extiende este concepto, permitiendo hallar una representación de los datos donde las distintas variables se organizan jerárquicamente según su relevancia. En concreto, permite hallar un sistema de coordenadas ortogonales formadas por combinaciones lineales de las originales, de forma tal que al ver los datos en este sistema, las nuevas variables (que ya no serán píxeles individuales, sino características comprendiendo a varios de ellos) queden ordenadas según la magnitud de sus varianzas. Los ejes de este sistema son lo que se conoce como \emph{componentes principales}.

Adicionalmente, es habitual hallar en datos reales con cierto grado de redundancia en su contenido, que la mayor parte de la varianza total del sistema se concentra en unas pocas componentes principales; esto permite descartar aquellas menos relevantes, reduciendo la dimensión de los datos con mínima pérdida de información, y exponiendo sus características más significativas.

\subsection{Descomposición SVD y relación con las componentes principales}

La descomposición en valores singulares (\emph{Singular Value Decomposition}) de una matriz \decMat{\X}{\M}{\N} esta compuesta por matrices \decMat{\U}{\M}{\M}, \decMat{\Sig}{\M}{\N} y \decMat{\V}{\N}{\N} cumpliendo:

$$\X = \U \Sig \V^{t}$$

donde \U y \V son ortogonales, \Sig es diagonal, y sus elementos $\sigma_i$ son no negativos y se encuentran ordenados decrecientemente.

Esta descomposición es útil en este caso ya que, de la teoría de del análisis de componentes principales, se desprende que el cambio de coordenadas entre el sistema original de los datos y el de sus componentes principales queda determinado por la matriz $\V^{t}$ de la descomposición en valores singulares de la matriz de covarianza de los datos.

En particular, como la matriz de covarianza es simétrica y semi-definida positiva por construcción \footnote{Si $A$ es la matriz de datos, $\A^{t} * \A = (\A^{t} * \A)^{t}$ y $\forall x \neq 0$, $x^t * A^t * A * x = \left \| A*x \right \|_2 \geq 0$}, también es diagonalizable y su descomposición SVD se puede tomar de forma equivalente a su descomposición $PDP^{-1}$.\footnote{Esta es la descomposición obtenida al diagonalizar la matriz.}

Por lo tanto, para obtener $\V^{t}$ es necesario calcular los autovectores de la matriz de covarianza de los datos. Sin embargo, si se quiere eliminar componentes que aporten poca información, reduciendo la dimensión de los datos a sus primeras $k$ componentes principales, es suficiente computar únicamente los $k$ autovectores dominantes; es decir, aquellos cuyos autovalores sean mayores en módulo.


\subsection{Método de las potencias y deflación}

EL algoritmo comienza con un vector de norma 1 aleatorio $b_0$. El método utiliza la siguiente iteración: 

$$b_{k+1} = \frac{Ab_k}{\|Ab_k\|}$$

donde A es la matriz de entrada.

Bajo la suposiciones: 
\begin{itemize}
  \item A tiene un autovalor que es estrictamente mayor en magnitud a los otro autovalores
  \item El vector tiene una componente distinta de cero en la dirección del autovector asociado al autovalor principal.
\end{itemize}

Entonces una subsucesión de $b_k$ converge al autovector asociado al autovalor dominante.
 
 
\subsection{Deflación}
Necesitamos encontrar $k$ autovectores, pero el método de la potencia solo permite encontrar el dominante. Para resolver este problema, 
luego de haber encontrado un un autovector, se realiza una transformación llamada deflación,
que elimina el autovalor dominante de forma que podamos encontrar el siguiente.
 
El algoritmo es el siguiente: \\

Para cada $i=1$ a $k$: \\
\indent \indent$v_i \leftarrow MetodoPotencia(A)$ \\
\indent \indent$\lambda_i \leftarrow (v_i^t A v_i) / ( v_i^t v_i)$\\
\indent \indent$A \leftarrow A - \lambda_i v_i v_i^t$ \\

 